{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "datasets: <RepeatDataset shapes: ((None, 28, 28), (None,)), types: (tf.float32, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "import  tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "(xs, ys),_ = datasets.mnist.load_data()\n",
    "print('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n",
    "\n",
    "xs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.  # still same shape\n",
    "db = tf.data.Dataset.from_tensor_slices((xs,ys))\n",
    "db = db.batch(32).repeat(10)\n",
    "print('datasets:', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  2570      \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = Sequential([layers.Dense(256, activation='relu'),   # (784 + 1) * 256\n",
    "                     layers.Dense(256, activation='relu'),   # 257 * 256\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "network.build(input_shape=(None, 28*28))\n",
    "network.summary()\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "acc_meter = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.1549545526504517 acc: 0.0625\n",
      "200 loss: 0.43066316843032837 acc: 0.65515625\n",
      "400 loss: 0.36594605445861816 acc: 0.8340625\n",
      "600 loss: 0.3201281428337097 acc: 0.8645313\n",
      "800 loss: 0.2777119576931 acc: 0.89234376\n",
      "1000 loss: 0.30878007411956787 acc: 0.891875\n",
      "1200 loss: 0.2879890203475952 acc: 0.9109375\n",
      "1400 loss: 0.24802908301353455 acc: 0.9184375\n",
      "1600 loss: 0.21240770816802979 acc: 0.909375\n",
      "1800 loss: 0.17359581589698792 acc: 0.9323437\n",
      "2000 loss: 0.2074495553970337 acc: 0.94109374\n",
      "2200 loss: 0.13924705982208252 acc: 0.93296874\n",
      "2400 loss: 0.2214827686548233 acc: 0.9315625\n",
      "2600 loss: 0.19837629795074463 acc: 0.93828124\n",
      "2800 loss: 0.14539489150047302 acc: 0.93703127\n",
      "3000 loss: 0.21524688601493835 acc: 0.934375\n",
      "3200 loss: 0.1776341199874878 acc: 0.9398438\n",
      "3400 loss: 0.14224880933761597 acc: 0.9365625\n",
      "3600 loss: 0.12950338423252106 acc: 0.938125\n",
      "3800 loss: 0.17978161573410034 acc: 0.9575\n",
      "4000 loss: 0.20065954327583313 acc: 0.953125\n",
      "4200 loss: 0.157958984375 acc: 0.94671875\n",
      "4400 loss: 0.14064942300319672 acc: 0.9484375\n",
      "4600 loss: 0.19524234533309937 acc: 0.9471875\n",
      "4800 loss: 0.13725027441978455 acc: 0.94578123\n",
      "5000 loss: 0.12550362944602966 acc: 0.95234376\n",
      "5200 loss: 0.2091810703277588 acc: 0.9482812\n",
      "5400 loss: 0.2008533477783203 acc: 0.9482812\n",
      "5600 loss: 0.09940659254789352 acc: 0.96234375\n",
      "5800 loss: 0.15102088451385498 acc: 0.9596875\n",
      "6000 loss: 0.12630167603492737 acc: 0.9560937\n",
      "6200 loss: 0.16386929154396057 acc: 0.9534375\n",
      "6400 loss: 0.09050830453634262 acc: 0.95640624\n",
      "6600 loss: 0.1222795844078064 acc: 0.9529688\n",
      "6800 loss: 0.09989449381828308 acc: 0.9565625\n",
      "7000 loss: 0.08676080405712128 acc: 0.9582813\n",
      "7200 loss: 0.29415175318717957 acc: 0.94734377\n",
      "7400 loss: 0.13923433423042297 acc: 0.96125\n",
      "7600 loss: 0.15602198243141174 acc: 0.9710938\n",
      "7800 loss: 0.08297159522771835 acc: 0.96046877\n",
      "8000 loss: 0.14378923177719116 acc: 0.9615625\n",
      "8200 loss: 0.07722252607345581 acc: 0.96125\n",
      "8400 loss: 0.0714358538389206 acc: 0.95890623\n",
      "8600 loss: 0.1003141775727272 acc: 0.95875\n",
      "8800 loss: 0.1496007740497589 acc: 0.9634375\n",
      "9000 loss: 0.11565371602773666 acc: 0.956875\n",
      "9200 loss: 0.06552170217037201 acc: 0.9578125\n",
      "9400 loss: 0.06449409574270248 acc: 0.97375\n",
      "9600 loss: 0.1718541979789734 acc: 0.9690625\n",
      "9800 loss: 0.057454030960798264 acc: 0.965\n",
      "10000 loss: 0.15742118656635284 acc: 0.963125\n",
      "10200 loss: 0.1024683341383934 acc: 0.9635937\n",
      "10400 loss: 0.12391510605812073 acc: 0.9590625\n",
      "10600 loss: 0.06728222966194153 acc: 0.96796876\n",
      "10800 loss: 0.1589890718460083 acc: 0.96296877\n",
      "11000 loss: 0.10151726007461548 acc: 0.95890623\n",
      "11200 loss: 0.11207440495491028 acc: 0.9703125\n",
      "11400 loss: 0.08954986184835434 acc: 0.9735938\n",
      "11600 loss: 0.12960819900035858 acc: 0.96671873\n",
      "11800 loss: 0.08247070759534836 acc: 0.9659375\n",
      "12000 loss: 0.08212468028068542 acc: 0.9665625\n",
      "12200 loss: 0.06196056678891182 acc: 0.96421874\n",
      "12400 loss: 0.11939755082130432 acc: 0.96671873\n",
      "12600 loss: 0.14808735251426697 acc: 0.9665625\n",
      "12800 loss: 0.10793358087539673 acc: 0.96125\n",
      "13000 loss: 0.08469383418560028 acc: 0.96765625\n",
      "13200 loss: 0.15247592329978943 acc: 0.97453123\n",
      "13400 loss: 0.09399468451738358 acc: 0.97125\n",
      "13600 loss: 0.08610615879297256 acc: 0.9685938\n",
      "13800 loss: 0.10160704702138901 acc: 0.9671875\n",
      "14000 loss: 0.05806977301836014 acc: 0.96703124\n",
      "14200 loss: 0.1839890480041504 acc: 0.96796876\n",
      "14400 loss: 0.0827939435839653 acc: 0.970625\n",
      "14600 loss: 0.15704435110092163 acc: 0.96453124\n",
      "14800 loss: 0.07522265613079071 acc: 0.9659375\n",
      "15000 loss: 0.09713902324438095 acc: 0.97734374\n",
      "15200 loss: 0.093255415558815 acc: 0.9742187\n",
      "15400 loss: 0.10423135757446289 acc: 0.96984375\n",
      "15600 loss: 0.0758642703294754 acc: 0.97046876\n",
      "15800 loss: 0.09152813255786896 acc: 0.9696875\n",
      "16000 loss: 0.12116831541061401 acc: 0.966875\n",
      "16200 loss: 0.09851744771003723 acc: 0.97125\n",
      "16400 loss: 0.09686195850372314 acc: 0.97203124\n",
      "16600 loss: 0.06398342549800873 acc: 0.963125\n",
      "16800 loss: 0.08733499050140381 acc: 0.975\n",
      "17000 loss: 0.10378576815128326 acc: 0.9775\n",
      "17200 loss: 0.04642823338508606 acc: 0.9717187\n",
      "17400 loss: 0.08549237996339798 acc: 0.9715625\n",
      "17600 loss: 0.09297598898410797 acc: 0.9715625\n",
      "17800 loss: 0.062185849994421005 acc: 0.97015625\n",
      "18000 loss: 0.07046525180339813 acc: 0.9725\n",
      "18200 loss: 0.07909560203552246 acc: 0.97328126\n",
      "18400 loss: 0.06075819954276085 acc: 0.96875\n",
      "18600 loss: 0.06141062453389168 acc: 0.97015625\n"
     ]
    }
   ],
   "source": [
    "# use GradientTape and Keras.Model.train()\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        # [b, 10]\n",
    "        loss = tf.square(out-y_onehot)\n",
    "        # [b]\n",
    "        loss = tf.reduce_sum(loss) / 32\n",
    "\n",
    "\n",
    "    acc_meter.update_state(tf.argmax(out, axis=1), y)\n",
    "\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 200==0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n",
    "        acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
