{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "assert tf.__version__.startswith('2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def swish(x):\n",
    "    return x * tf.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class namespace():\n",
    "    pass\n",
    "args = namespace()\n",
    "args.n_ctx = 512\n",
    "args.n_embd = 768\n",
    "args.n_head = 12\n",
    "args.n_layer = 12\n",
    "args.embd_pdrop = 0.1\n",
    "args.attn_pdrop = 0.1\n",
    "args.resid_pdrop = 0.1\n",
    "args.clf_pdrop = 0.1\n",
    "args.l2 = 0.1\n",
    "args.n_transfer = 12\n",
    "args.lm_coef = 0.5\n",
    "args.b1 = 0.9\n",
    "args.b2 = 0.999\n",
    "args.e = 1e-8\n",
    "args.n_valid = 374\n",
    "args.afn = gelu\n",
    "\n",
    "zeros_init = keras.initializers.Zeros()\n",
    "ones_init = keras.initializers.Ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(keras.Model):\n",
    "    \"\"\"Construct a layernorm module in the OpenAI style (epsilon inside the square root).\"\"\"\n",
    "\n",
    "    def __init__(self, n_state=768, e=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.g = self.add_weight(shape=[n_state], initializer=ones_init)\n",
    "        self.b = self.add_weight(shape=[n_state], initializer=zeros_init)\n",
    "        self.e = e\n",
    "    \n",
    "    def call(self, x):\n",
    "        u = tf.reduce_mean(x, -1, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.pow(x-u, 2), -1, keepdims=True)\n",
    "        x = (x-u) / tf.sqrt(s+self.e)\n",
    "        return self.g * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(keras.Model):\n",
    "\n",
    "    def __init__(self, nf=768*3, rf=1, nx=768):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.rf = rf\n",
    "        self.nf = nf\n",
    "        if rf == 1: # faster 1x1 conv\n",
    "            self.w = self.add_weight(shape=[nx,nf], initializer=keras.initializers.RandomNormal(stddev=0.02))\n",
    "            self.b = self.add_weight(shape=[nf], initializer=zeros_init)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.rf == 1:\n",
    "            size_out = list(x.shape[:-1]) + [self.nf]\n",
    "            x = tf.matmul(tf.reshape(x, [-1, x.shape[-1]]), self.w) + self.b\n",
    "            x = tf.reshape(x, size_out)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.Model):\n",
    "    \n",
    "    def __init__(self, nx=768, n_ctx=512, cfg=args, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx # in Attention: n_state = 768 (nx=n_emb)\n",
    "         # [switch nx => n_state from Block to Attention to keep identical to openai implem]\n",
    "        assert n_state % cfg.n_head == 0\n",
    "        self.b = self.add_weight(shape=[1, 1, n_ctx, n_ctx], initializer=ones_init) # register buffer\n",
    "        self.b.assign(tf.linalg.LinearOperatorLowerTriangular(self.b).to_dense())\n",
    "        self.n_head = cfg.n_head\n",
    "        #self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state*3, 1, nx)\n",
    "        self.c_proj = Conv1D(n_state, 1, nx)\n",
    "        self.attn_dropout = keras.layers.Dropout(cfg.attn_pdrop)\n",
    "        self.resid_dropout = keras.layers.Dropout(cfg.resid_pdrop)\n",
    "    \n",
    "    def _attn(self, q, k, v):\n",
    "        w = tf.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / tf.sqrt(tf.cast(v.shape[-1], tf.float32))\n",
    "        # self.b may be larger than w, so we need to crop it\n",
    "        b = self.b[:, :, :w.shape[-2], :w.shape[-1]]\n",
    "        w = w * b + 1e-9 * (1 - b)\n",
    "        w = tf.nn.softmax(w, -1)\n",
    "        return tf.matmul(w, v)\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x = tf.transpose(x, [0,2,1,3])\n",
    "        new_x_shape = list(x.shape[:-2]) + [x.shape[-2]*x.shape[-1]]\n",
    "        return tf.reshape(x, new_x_shape) # in openai implem: fct merge_states\n",
    "    \n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = list(x.shape[:-1]) + [self.n_head, x.shape[-1]//self.n_head]\n",
    "        x = tf.reshape(x, new_x_shape) # in openai implem: fct split_states\n",
    "        if k:\n",
    "            return tf.transpose(x, [0,2,3,1])\n",
    "        else:\n",
    "            return tf.transpose(x, [0,2,1,3])\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = tf.split(x, 3, axis=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "        a = self._attn(query, key, value)\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        a = self.resid_dropout(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(keras.Model):\n",
    "\n",
    "    def __init__(self, n_state=3072, cfg=args): # n_state=3072 (4*n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = cfg.n_embd\n",
    "        self.c_fc = Conv1D(n_state, 1, nx)\n",
    "        self.c_proj = Conv1D(nx, 1, n_state)\n",
    "        self.act = cfg.afn\n",
    "        self.dropout = keras.layers.Dropout(cfg.resid_pdrop)\n",
    "    \n",
    "    def call(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return self.dropout(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(keras.Model):\n",
    "\n",
    "    def __init__(self, n_ctx=512, cfg=args, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = cfg.n_embd\n",
    "        self.attn = Attention(nx, n_ctx, cfg, scale)\n",
    "        self.ln_1 = LayerNorm(nx)\n",
    "        self.mlp = MLP(4 * nx, cfg)\n",
    "        self.ln_2 = LayerNorm(nx)\n",
    "    \n",
    "    def call(self, x):\n",
    "        a = self.attn(x)\n",
    "        n = self.ln_1(x + a)\n",
    "        m = self.mlp(n)\n",
    "        h = self.ln_2(n + m)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "    \"\"\" Transformer model \"\"\"\n",
    "\n",
    "    def __init__(self, cfg=args, vocab=40558, n_ctx=512):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embed = keras.layers.Embedding(vocab, cfg.n_embd)\n",
    "        self.embed.build([1])\n",
    "        self.drop = keras.layers.Dropout(cfg.embd_pdrop)\n",
    "        self.h = [Block(n_ctx, cfg, scale=True) for _ in range(cfg.n_layer)]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, [-1,x.shape[-2],x.shape[-1]])\n",
    "        e = self.drop(self.embed(x))\n",
    "        # add the position information to input embeddings\n",
    "        h = tf.reduce_sum(e, 2)\n",
    "        for block in self.h:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class LMHead(keras.Model):\n",
    "    \"\"\" Language Model Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, model, cfg=args, trunc_and_reshape=True):\n",
    "        super(LMHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        embed_shape = model.embed.weights[0].shape\n",
    "        self.embed = model.embed.weights[0]\n",
    "        self.decoder = lambda x: tf.matmul(x, tf.transpose(self.embed))\n",
    "        self.trunc_and_reshape = trunc_and_reshape  # XD\n",
    "\n",
    "    def call(self, h):\n",
    "        # Truncated Language modeling logits (we remove the last token)\n",
    "        h_trunc = tf.reshape(h[:, :-1], [-1, self.n_embd]) \\\n",
    "            if self.trunc_and_reshape else h  # XD\n",
    "        lm_logits = self.decoder(h_trunc)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "class MultipleChoiceHead(keras.Model):\n",
    "    \"\"\" Classifier Head for the transformer \"\"\"\n",
    "\n",
    "    def __init__(self, clf_token=40480, cfg=args):\n",
    "        super(MultipleChoiceHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.n_ctx = cfg.n_ctx\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = keras.layers.Dropout(cfg.clf_pdrop, [1, 2, cfg.n_embd, 1]) # might need to change the 1s to smth else\n",
    "        self.linear = keras.layers.Dense(1, input_shape=[cfg.n_embd], \n",
    "            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n",
    "        self.linear.build([cfg.n_embd])\n",
    "\n",
    "    def call(self, h, x):\n",
    "        # Classification logits\n",
    "        clf_h = tf.reshape(h, [-1, self.n_embd])\n",
    "        flat = tf.reshape(x[..., 0], [-1])\n",
    "        clf_h = tf.boolean_mask(clf_h, tf.equal(flat, self.clf_token))\n",
    "        clf_h = tf.reshape(clf_h, [-1, x.shape[1], self.n_embd, 1])\n",
    "        # This double transposition is there to replicate the behavior\n",
    "        # of the noise_shape argument in the tensorflow\n",
    "        # implementation.  For more details, see\n",
    "        # https://github.com/huggingface/pytorch-openai-transformer-lm/issues/11\n",
    "        # clf_h = self.dropout(clf_h.transpose(1, 2)).transpose(1, 2)\n",
    "        clf_h = self.dropout(clf_h)\n",
    "        clf_h = tf.reshape(clf_h, [-1, self.n_embd])\n",
    "        clf_logits = self.linear(clf_h)\n",
    "\n",
    "        return tf.reshape(clf_logits, [-1, x.shape[1]])\n",
    "\n",
    "\n",
    "class ClfHead(keras.Model):\n",
    "    \"\"\"Classification Head for the transformer\n",
    "\n",
    "    TODO: test this class.\"\"\"\n",
    "    def __init__(self, clf_token=40480, cfg=args, n_class=10):\n",
    "        super(ClfHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = keras.layers.Dropout(cfg.clf_pdrop)\n",
    "        self.linear = keras.layers.Dense(n_class, input_shape=[cfg.n_embd], \n",
    "            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n",
    "\n",
    "    def call(self, h, x):\n",
    "        clf_h = tf.reshape(h, [-1, self.n_embd])\n",
    "        flat = tf.reshape(x[..., 0], [-1])\n",
    "        clf_h = clf_h[flat == self.clf_token, :]\n",
    "        clf_h = tf.boolean_mask(clf_h, tf.equal(flat, self.clf_token))\n",
    "        clf_h = self.dropout(clf_h)\n",
    "        clf_logits = self.linear(clf_h)\n",
    "\n",
    "        return clf_logits\n",
    "\n",
    "\n",
    "class SimilarityHead(keras.Model):\n",
    "    \"\"\" Similarity Head for the transformer\n",
    "\n",
    "        TODO: test this class.\"\"\"\n",
    "    def __init__(self, clf_token=40480, cfg=args):\n",
    "        super(SimilarityHead, self).__init__()\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.clf_token = clf_token\n",
    "        self.dropout = keras.layers.Dropout(cfg.clf_pdrop)\n",
    "        self.linear = keras.layers.Dense(n_class, input_shape=[cfg.n_embd], \n",
    "            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n",
    "            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n",
    "\n",
    "    def call(self, h, x):\n",
    "        sim_h = tf.reshape(h, [-1, self.n_embd])\n",
    "        flat = tf.reshape(x[..., 0], [-1])\n",
    "        sim_h = tf.boolean_mask(sim_h, tf.equal(flat, self.clf_token))\n",
    "        sim_h = self.dropout(sim_h)\n",
    "        sim_h = tf.reduce_sum(sim_h, 1)\n",
    "        sim_logits = self.linear(sim_h)\n",
    "\n",
    "        return sim_logits\n",
    "\n",
    "\n",
    "class LMModel(keras.Model):\n",
    "    \"\"\" Transformer with language model head only \"\"\"\n",
    "    def __init__(self, cfg=args, vocab=40990, n_ctx=512, return_probs=False):\n",
    "        super(LMModel, self).__init__()\n",
    "        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
    "        self.lm_head = LMHead(self.transformer, cfg, trunc_and_reshape=False)\n",
    "        self.return_probs = return_probs\n",
    "        if self.return_probs:\n",
    "            pos_emb_mask = tf.zeros([1, 1, vocab]) # register buffer\n",
    "            pos_emb_mask[:, :, -n_ctx:] = -1e12\n",
    "\n",
    "    def call(self, x):\n",
    "        h = self.transformer(x)\n",
    "        lm_logits = self.lm_head(h)\n",
    "        if self.return_probs:\n",
    "            lm_logits = tf.nn.softmax(lm_logits + self.pos_emb_mask, -1)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "class DoubleHeadModel(keras.Model):\n",
    "    \"\"\" Transformer with language model and task specific heads \"\"\"\n",
    "    def __init__(self, cfg=args, clf_token=40480, task_head_type='multiple_choice', vocab=40990, n_ctx=512):\n",
    "        super(DoubleHeadModel, self).__init__()\n",
    "        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
    "        self.lm_head = LMHead(self.transformer, cfg)\n",
    "        if isinstance(task_head_type, str):\n",
    "            if task_head_type == 'multiple_choice':\n",
    "                self.task_head = MultipleChoiceHead(clf_token, cfg)\n",
    "            elif task_head_type == 'similarity':\n",
    "                self.task_head = SimilarityHead(clf_token, cfg)\n",
    "            elif task_head_type == 'inference':\n",
    "                # the three classes correspond to entailment, contradiction and neutral.\n",
    "                self.task_head = ClfHead(clf_token, cfg, 3)\n",
    "            else:\n",
    "                raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                                 \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                                 f\"got {task_head_type}.\")\n",
    "        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n",
    "             task_head_type[0] == 'classification':\n",
    "            n_class = task_head_type[1]\n",
    "            self.task_head = ClfHead(clf_token, cfg, n_class)\n",
    "        else:\n",
    "            raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
    "                             \"'similarity', 'inference' or ('classification', n_class) \"\n",
    "                             f\"got {task_head_type}.\")\n",
    "\n",
    "    def call(self, x):\n",
    "        h = self.transformer(x)\n",
    "        lm_logits = self.lm_head(h)\n",
    "        task_logits = self.task_head(h, x)\n",
    "\n",
    "        return lm_logits, task_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
